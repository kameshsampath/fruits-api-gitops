{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Fruits API GitOps demostrates how to use GitOps principles in building and publishing your API using Gloo Edge . As part of this demo we will build a simple Java REST API using Quarkus and publish them them using Gloo Portal .","title":"Overview"},{"location":"clean-up/","text":"At the end of this chapter you would have, Delete minikube clusters Ensure Environment \u00b6 Navigate to Tutorial home cd $DEMO_HOME Set cluster environment variables source $DEMO_WORK_DIR /.envrc Destroy minikube clusters \u00b6 make clean-up","title":"Cleanup"},{"location":"clean-up/#ensure-environment","text":"Navigate to Tutorial home cd $DEMO_HOME Set cluster environment variables source $DEMO_WORK_DIR /.envrc","title":"Ensure Environment"},{"location":"clean-up/#destroy-minikube-clusters","text":"make clean-up","title":"Destroy minikube clusters"},{"location":"deploy/","text":"At the end of chapter you would have, Deployed Tekton tasks and pipelines reuqired to build Fruits API Deployed the Argocd Application to deploy Fruits API Configure the Fruits API Webhook with Gitea Ensure Environment \u00b6 source $DEMO_WORK_DIR /.envrc Pipelines \u00b6 As part of the demo we will be using tektoncd pipelines to build and push the image to the container registry. Deploy Tektoncd Tasks \u00b6 The following Tektoncd community tasks will be used by the FruitsAPI as part of the application build, since tektoncd does not install these out of the box we wil install them manually, maven \u00b6 tkn hub install task maven \\ --version = 0 .2 \\ --context = \" $CLUSTER1 \" git-clone \u00b6 tkn hub install task git-clone \\ --version = 0 .5 \\ --context = \" $CLUSTER1 \" buildah \u00b6 tkn hub install task buildah \\ --version = 0 .3 \\ --context = \" $CLUSTER1 \" openshift-client \u00b6 tkn hub install task openshift-client \\ --version = 0 .2 \\ --context = \" $CLUSTER1 \" Create Tektoncd pipelines \u00b6 As the piplines will build and push the container image to ghcr.io it is required to have the following two variables set in your enviroment, export GITHUB_USERNAME = <your github username> export GHCR_PASSWORD = <your Github PAT> Important GHCR_PASSWORD is the GitHub PAT with the following permissions: repo read access package write access Create the pipeline fruits-api-deploy , kustomize build pipelines \\ | envsubst \\ | kubectl apply --context $CLUSTER1 -f - Tekton Triggers \u00b6 The Tekton Triggers take care of rebuilding the application as and when the new code is committed into the Git repository. Get the gloo gateway-proxy LoadBalancer ip to configure the gloo routes and the same will be used to configure the Git Webhooks later, export GLOO_GATEWAY_PROXY_IP = \" $( kubectl --context = \" $CLUSTER1 \" -n gloo-system get svc gateway-proxy -ojsonpath = '{.status.loadBalancer.ingress[*].ip}' ) \" Verify to see if it has value set, echo \" ${ GLOO_GATEWAY_PROXY_IP } \" Create Tekton Triggers that will run the image build once changes are pushed to fruits-api, kustomize build triggers \\ | envsubst | kubectl apply --context $CLUSTER1 -f - Wait for the Gitea event listener webhook to be running, kubectl --context = $CLUSTER1 \\ rollout status deploy/el-gitea-webhook --timeout = 120s Create the dev remote to Gitea \u00b6 export FRUITS_API_GITOPS_REPO_URL = \"https://gitea- $( kubectl --context = \" $MGMT \" -n gitea get svc gateway-proxy -ojsonpath = '{.status.loadBalancer.ingress[*].ip}' ) .nip.io/gitea/fruits-api-gitops.git\" git remote add dev $FRUITS_API_GITOPS_REPO_URL Commit and push the local code to the Gitea repository. git commit -a -m \"Repo Init\" git push dev main The default Gitea credentials is gitea/password . GitOps with Argocd \u00b6 Add Gitea Repository to Argocd \u00b6 As the Gitea repository we will be using local and uses self signed certificates, let us configure that in Argocd to skip sslVerify , Login to Argocd, # make sure we are in mgmt kubernetes context kubectl config use-context mgmt argocd login --insecure \\ \" $( yq e '.serviceUrl' $DEMO_WORK_DIR /argocd_details.yaml | cut -d '/' -f3 ) \" \\ --username \" ${ ARGOCD_ADMIN_USERNAME } \" \\ --password \" ${ ARGOCD_ADMIN_PASSWORD } \" Add the local Gitea repository, argocd repo add \" $( yq e '.gitea_url' work/gitea_details.yaml ) / ${ GITEA_USERNAME } /fruits-api-gitops.git\" --username \" ${ GITEA_USERNAME } \" --password \" ${ GITEA_PASSWORD } \" --insecure-skip-server-verification Create Application \u00b6 Query the cluster1 info to get the cluster API URL and run the following command to create fruits-api ArgoCD application. # Ensures colors are also removed form output export TARGET_CLUSTER = \" $( kubectl --context = \" $CLUSTER1 \" cluster-info | sed 's/\\x1b\\[[0-9;]*m//g' | awk 'NR==1{print $7}' ) \" yq eval \\ '.spec.destination.server = strenv(TARGET_CLUSTER) | .spec.source.repoURL = strenv(FRUITS_API_GITOPS_REPO_URL)' \\ manifests/app/app.yaml \\ | kubectl apply --context = \" $MGMT \" -n argocd -f - The Argocd application will apply the helm chart $DEMO_HOME/charts/fruits-api using Helm values from $DEMO_HOME/helm_vars/fruits-api/values.yaml . The helm values supports by the chart are, replicaCount : 1 # Gloo Portal Configuration enablePortal : true # Enable Gloo Portal RBAC enableRBAC : false # The Portal suffix to use with Gloo Dev Portal # e.g. api.kameshs.me, portal.kamesh.me portalDomainSuffix : kameshs.me # DB Configuration to be used with Fruits API postgresql : global : postgresql : postgresqlDatabase : fruitsdb postgresqlUsername : postgres postgresqlPassword : password servicePort : 5432 # the ConfigMap that will hold the DB init script initdbScriptsConfigMap : postgres-schema # The fruits-api image to be used in the deployment image : name : ghcr.io/kameshsampath/fruits-api pullPolicy : IfNotPresent tag : \"\" imagePullSecrets : [ ] nameOverride : \"\" fullnameOverride : \"\" serviceAccount : # Specifies whether a service account should be created create : true # Annotations to add to the service account annotations : { } # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name : \"\" podAnnotations : { } podSecurityContext : { } # fsGroup: 2000 securityContext : { } # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 # The fruits-api Kubernetes Service service : type : ClusterIP port : 8080 # the pod resource limit and requests resources : limits : cpu : 512m memory : 1Gi requests : cpu : 256m memory : 512Mi # Kubernetes NodeSelector Labels nodeSelector : { } # Kubernetes Pod Tolerations tolerations : [ ] # Kubernetes Pod Affinity affinity : { } Routes \u00b6 As we have already deployed the Gloo Edge, the service should have been auto discovered via Gloo. Let us run the following command to verify it, kubectl --context = \" ${ CLUSTER1 } \" get upstream default-fruits-api-8080 \\ -n gloo-system -o yaml Lets create route to access the API, cat <<EOF | kubectl --context=\"${CLUSTER1}\" apply -f - apiVersion: gateway.solo.io/v1 kind: VirtualService metadata: name: fruits-api-http namespace: gloo-system spec: virtualHost: domains: - fruits-api-${GLOO_GATEWAY_PROXY_IP}.nip.io routes: - matchers: - prefix: / routeAction: single: upstream: name: default-fruits-api-8080 namespace: gloo-system EOF Now calling the service http fruits-api-192.168.64.100.nip.io/api/fruits will return a list of fruits. Lets delete the test route we have created as we will be using the Gloo Developer Portal to access the API. kubectl --context = \" ${ CLUSTER1 } \" delete vs -n gloo-system fruits-api-http Portal \u00b6 To enable porta edit and update the $DEMO_HOME/helm_vars/fruits-api/values.yaml enablePortal to true . Commit and push the code to git repository to see the Argocd synchronizing the application to create the new Gloo Portal resources, yq -i e '.enablePortal=true' $DEMO_HOME /helm_vars/fruits-api/values.yaml git commit $DEMO_HOME /helm_vars/fruits-api/values.yaml -m \"Enable Portal\" git push dev main Wait for Argocd to synchroize the commit, once the commit is synchronized you should see the Gloo Portal resources created in the default namespace, kubectl --context = \" ${ CLUSTER1 } \" get apidocs,apiproducts,portal,environment NAME AGE apidoc.portal.gloo.solo.io/apidoc-v1-fruits 2m21s NAME AGE apiproduct.portal.gloo.solo.io/fruits-product 2m21s NAME AGE portal.portal.gloo.solo.io/fruits-portal 2m21s NAME AGE environment.portal.gloo.solo.io/dev 2m21s Now you can open the portal on your browser using the domain http://portal.kameshs.me Tip Update your /etc/hosts as shown to allow accessing the portal using domain names 192 .168.64.100 api.kameshs.me api 192 .168.64.100 portal.kameshs.me portal Where 192.168.64.100 is the minikube -pcluster1 ip Enable Authentication \u00b6 As you have observed by navigating to the APIs that all APIs are read only. To make the portal accessible we need to enable authentication. To enable porta edit and update the $DEMO_HOME/helm_vars/fruits-api/values.yaml enablePortal to true . Commit and push the code to git repository to see the Argocd synchronizing the application to create the new Gloo Portal resources, yq -i e '.enableRBAC=true' $DEMO_HOME /helm_vars/fruits-api/values.yaml git commit $DEMO_HOME /helm_vars/fruits-api/values.yaml -m \"Enable Portal RBAC\" git push dev main Now hit the login button and try login to the portal using the user dev1 and password mysecurepassword . Now when you check the APIs section it has the Try out option that allows you try the APIs. When you try the API from CLI, http api.kameshs.me/fruits/v1/api/fruits You should see you are not authorized, Let use geneate an API Key for dev1 , http api.kameshs.me/fruits/v1/api/fruits 'api-key: $API_KEY' Monetization \u00b6 Lets enable access to Admin console of the portal, kubectl --context = \" ${ CLUSTER1 } \" -n gloo-portal port-forward svc/gloo-portal-admin-server 8080 DB Setup \u00b6 Lets create the requests table, kubectl --context = \" ${ CLUSTER1 } \" get cm \\ -n gloo-system postgres-schema -o yaml \\ | yq e '.data[\"init-schema.sql\"]' > /tmp/gloo-portal-db.sql Deploy the DBAdminer utility, kubectl --context = \" ${ CLUSTER1 } \" apply -k mainfests/dbadminer kubectl --context = \" ${ CLUSTER1 } \" rollout status deploy/db-adminer Open the DB Adminer via the browser, export DB_ADMINER_IP = $( kubectl --context = \" ${ CLUSTER1 } \" get svc db-adminer -ojsonpath = '{.status.loadBalancer.ingress[*].ip}' ) Open the url http://$(DB_ADMINER_IP):8080 , use the Posygresql as database with user id postgres and password password . Then run the following SQL command to create the requests tabl using the sql /tmp/gloo-portal-db.sql Lets fire some requests to the API to generate the API calls graph, for i in { 1 ..5 } ; do http api.kameshs.me/fruits/v1/api/fruits 'api-key: $API_KEY' done","title":"Deploy Microservices"},{"location":"deploy/#ensure-environment","text":"source $DEMO_WORK_DIR /.envrc","title":"Ensure Environment"},{"location":"deploy/#pipelines","text":"As part of the demo we will be using tektoncd pipelines to build and push the image to the container registry.","title":"Pipelines"},{"location":"deploy/#deploy-tektoncd-tasks","text":"The following Tektoncd community tasks will be used by the FruitsAPI as part of the application build, since tektoncd does not install these out of the box we wil install them manually,","title":"Deploy Tektoncd Tasks"},{"location":"deploy/#maven","text":"tkn hub install task maven \\ --version = 0 .2 \\ --context = \" $CLUSTER1 \"","title":"maven"},{"location":"deploy/#git-clone","text":"tkn hub install task git-clone \\ --version = 0 .5 \\ --context = \" $CLUSTER1 \"","title":"git-clone"},{"location":"deploy/#buildah","text":"tkn hub install task buildah \\ --version = 0 .3 \\ --context = \" $CLUSTER1 \"","title":"buildah"},{"location":"deploy/#openshift-client","text":"tkn hub install task openshift-client \\ --version = 0 .2 \\ --context = \" $CLUSTER1 \"","title":"openshift-client"},{"location":"deploy/#create-tektoncd-pipelines","text":"As the piplines will build and push the container image to ghcr.io it is required to have the following two variables set in your enviroment, export GITHUB_USERNAME = <your github username> export GHCR_PASSWORD = <your Github PAT> Important GHCR_PASSWORD is the GitHub PAT with the following permissions: repo read access package write access Create the pipeline fruits-api-deploy , kustomize build pipelines \\ | envsubst \\ | kubectl apply --context $CLUSTER1 -f -","title":"Create Tektoncd pipelines"},{"location":"deploy/#tekton-triggers","text":"The Tekton Triggers take care of rebuilding the application as and when the new code is committed into the Git repository. Get the gloo gateway-proxy LoadBalancer ip to configure the gloo routes and the same will be used to configure the Git Webhooks later, export GLOO_GATEWAY_PROXY_IP = \" $( kubectl --context = \" $CLUSTER1 \" -n gloo-system get svc gateway-proxy -ojsonpath = '{.status.loadBalancer.ingress[*].ip}' ) \" Verify to see if it has value set, echo \" ${ GLOO_GATEWAY_PROXY_IP } \" Create Tekton Triggers that will run the image build once changes are pushed to fruits-api, kustomize build triggers \\ | envsubst | kubectl apply --context $CLUSTER1 -f - Wait for the Gitea event listener webhook to be running, kubectl --context = $CLUSTER1 \\ rollout status deploy/el-gitea-webhook --timeout = 120s","title":"Tekton Triggers"},{"location":"deploy/#create-the-dev-remote-to-gitea","text":"export FRUITS_API_GITOPS_REPO_URL = \"https://gitea- $( kubectl --context = \" $MGMT \" -n gitea get svc gateway-proxy -ojsonpath = '{.status.loadBalancer.ingress[*].ip}' ) .nip.io/gitea/fruits-api-gitops.git\" git remote add dev $FRUITS_API_GITOPS_REPO_URL Commit and push the local code to the Gitea repository. git commit -a -m \"Repo Init\" git push dev main The default Gitea credentials is gitea/password .","title":"Create the dev remote to Gitea"},{"location":"deploy/#gitops-with-argocd","text":"","title":"GitOps with Argocd"},{"location":"deploy/#add-gitea-repository-to-argocd","text":"As the Gitea repository we will be using local and uses self signed certificates, let us configure that in Argocd to skip sslVerify , Login to Argocd, # make sure we are in mgmt kubernetes context kubectl config use-context mgmt argocd login --insecure \\ \" $( yq e '.serviceUrl' $DEMO_WORK_DIR /argocd_details.yaml | cut -d '/' -f3 ) \" \\ --username \" ${ ARGOCD_ADMIN_USERNAME } \" \\ --password \" ${ ARGOCD_ADMIN_PASSWORD } \" Add the local Gitea repository, argocd repo add \" $( yq e '.gitea_url' work/gitea_details.yaml ) / ${ GITEA_USERNAME } /fruits-api-gitops.git\" --username \" ${ GITEA_USERNAME } \" --password \" ${ GITEA_PASSWORD } \" --insecure-skip-server-verification","title":"Add Gitea Repository to Argocd"},{"location":"deploy/#create-application","text":"Query the cluster1 info to get the cluster API URL and run the following command to create fruits-api ArgoCD application. # Ensures colors are also removed form output export TARGET_CLUSTER = \" $( kubectl --context = \" $CLUSTER1 \" cluster-info | sed 's/\\x1b\\[[0-9;]*m//g' | awk 'NR==1{print $7}' ) \" yq eval \\ '.spec.destination.server = strenv(TARGET_CLUSTER) | .spec.source.repoURL = strenv(FRUITS_API_GITOPS_REPO_URL)' \\ manifests/app/app.yaml \\ | kubectl apply --context = \" $MGMT \" -n argocd -f - The Argocd application will apply the helm chart $DEMO_HOME/charts/fruits-api using Helm values from $DEMO_HOME/helm_vars/fruits-api/values.yaml . The helm values supports by the chart are, replicaCount : 1 # Gloo Portal Configuration enablePortal : true # Enable Gloo Portal RBAC enableRBAC : false # The Portal suffix to use with Gloo Dev Portal # e.g. api.kameshs.me, portal.kamesh.me portalDomainSuffix : kameshs.me # DB Configuration to be used with Fruits API postgresql : global : postgresql : postgresqlDatabase : fruitsdb postgresqlUsername : postgres postgresqlPassword : password servicePort : 5432 # the ConfigMap that will hold the DB init script initdbScriptsConfigMap : postgres-schema # The fruits-api image to be used in the deployment image : name : ghcr.io/kameshsampath/fruits-api pullPolicy : IfNotPresent tag : \"\" imagePullSecrets : [ ] nameOverride : \"\" fullnameOverride : \"\" serviceAccount : # Specifies whether a service account should be created create : true # Annotations to add to the service account annotations : { } # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name : \"\" podAnnotations : { } podSecurityContext : { } # fsGroup: 2000 securityContext : { } # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 # The fruits-api Kubernetes Service service : type : ClusterIP port : 8080 # the pod resource limit and requests resources : limits : cpu : 512m memory : 1Gi requests : cpu : 256m memory : 512Mi # Kubernetes NodeSelector Labels nodeSelector : { } # Kubernetes Pod Tolerations tolerations : [ ] # Kubernetes Pod Affinity affinity : { }","title":"Create Application"},{"location":"deploy/#routes","text":"As we have already deployed the Gloo Edge, the service should have been auto discovered via Gloo. Let us run the following command to verify it, kubectl --context = \" ${ CLUSTER1 } \" get upstream default-fruits-api-8080 \\ -n gloo-system -o yaml Lets create route to access the API, cat <<EOF | kubectl --context=\"${CLUSTER1}\" apply -f - apiVersion: gateway.solo.io/v1 kind: VirtualService metadata: name: fruits-api-http namespace: gloo-system spec: virtualHost: domains: - fruits-api-${GLOO_GATEWAY_PROXY_IP}.nip.io routes: - matchers: - prefix: / routeAction: single: upstream: name: default-fruits-api-8080 namespace: gloo-system EOF Now calling the service http fruits-api-192.168.64.100.nip.io/api/fruits will return a list of fruits. Lets delete the test route we have created as we will be using the Gloo Developer Portal to access the API. kubectl --context = \" ${ CLUSTER1 } \" delete vs -n gloo-system fruits-api-http","title":"Routes"},{"location":"deploy/#portal","text":"To enable porta edit and update the $DEMO_HOME/helm_vars/fruits-api/values.yaml enablePortal to true . Commit and push the code to git repository to see the Argocd synchronizing the application to create the new Gloo Portal resources, yq -i e '.enablePortal=true' $DEMO_HOME /helm_vars/fruits-api/values.yaml git commit $DEMO_HOME /helm_vars/fruits-api/values.yaml -m \"Enable Portal\" git push dev main Wait for Argocd to synchroize the commit, once the commit is synchronized you should see the Gloo Portal resources created in the default namespace, kubectl --context = \" ${ CLUSTER1 } \" get apidocs,apiproducts,portal,environment NAME AGE apidoc.portal.gloo.solo.io/apidoc-v1-fruits 2m21s NAME AGE apiproduct.portal.gloo.solo.io/fruits-product 2m21s NAME AGE portal.portal.gloo.solo.io/fruits-portal 2m21s NAME AGE environment.portal.gloo.solo.io/dev 2m21s Now you can open the portal on your browser using the domain http://portal.kameshs.me Tip Update your /etc/hosts as shown to allow accessing the portal using domain names 192 .168.64.100 api.kameshs.me api 192 .168.64.100 portal.kameshs.me portal Where 192.168.64.100 is the minikube -pcluster1 ip","title":"Portal"},{"location":"deploy/#enable-authentication","text":"As you have observed by navigating to the APIs that all APIs are read only. To make the portal accessible we need to enable authentication. To enable porta edit and update the $DEMO_HOME/helm_vars/fruits-api/values.yaml enablePortal to true . Commit and push the code to git repository to see the Argocd synchronizing the application to create the new Gloo Portal resources, yq -i e '.enableRBAC=true' $DEMO_HOME /helm_vars/fruits-api/values.yaml git commit $DEMO_HOME /helm_vars/fruits-api/values.yaml -m \"Enable Portal RBAC\" git push dev main Now hit the login button and try login to the portal using the user dev1 and password mysecurepassword . Now when you check the APIs section it has the Try out option that allows you try the APIs. When you try the API from CLI, http api.kameshs.me/fruits/v1/api/fruits You should see you are not authorized, Let use geneate an API Key for dev1 , http api.kameshs.me/fruits/v1/api/fruits 'api-key: $API_KEY'","title":"Enable Authentication"},{"location":"deploy/#monetization","text":"Lets enable access to Admin console of the portal, kubectl --context = \" ${ CLUSTER1 } \" -n gloo-portal port-forward svc/gloo-portal-admin-server 8080","title":"Monetization"},{"location":"deploy/#db-setup","text":"Lets create the requests table, kubectl --context = \" ${ CLUSTER1 } \" get cm \\ -n gloo-system postgres-schema -o yaml \\ | yq e '.data[\"init-schema.sql\"]' > /tmp/gloo-portal-db.sql Deploy the DBAdminer utility, kubectl --context = \" ${ CLUSTER1 } \" apply -k mainfests/dbadminer kubectl --context = \" ${ CLUSTER1 } \" rollout status deploy/db-adminer Open the DB Adminer via the browser, export DB_ADMINER_IP = $( kubectl --context = \" ${ CLUSTER1 } \" get svc db-adminer -ojsonpath = '{.status.loadBalancer.ingress[*].ip}' ) Open the url http://$(DB_ADMINER_IP):8080 , use the Posygresql as database with user id postgres and password password . Then run the following SQL command to create the requests tabl using the sql /tmp/gloo-portal-db.sql Lets fire some requests to the API to generate the API calls graph, for i in { 1 ..5 } ; do http api.kameshs.me/fruits/v1/api/fruits 'api-key: $API_KEY' done","title":"DB Setup"},{"location":"env-setup/","text":"authors: - Kamesh Sampath kamesh.sampath@hotmail.com date: 2021-12-06 At the end of this chapter you will have, Two minikube clusters mgmt and cluster1 mgmt cluster will have Argocd and Gitea git repository manager cluster will have Tektoncd and Gloo Edge and Portal deployed Ensure Poetry \u00b6 The project uses poetry to setup Python3 virtual environment to run ansible scripts. You can use pipx to install poetry by, pipx install poetry && pipx ensurepath Lets ensure poetry is setup correctly, running the following command should return the version of the installed poetry utility. poetry --version Let us configure poetry to create the python3 virutalenv in the project directory $DEMO_HOME , poetry config virtualenvs.in-project true To create the virutal environment run the following command, poetry install The command will instal all the required python modules in the $DEMO_HOME/.venv . Install the ansible roles and collections that will be used by the playbooks, make install-roles-and-collections Tip Setup Kubernetes Environment \u00b6 With Ansible environmnt ready we are all good to setup demo environment with required components installed, Minikube clusters \u00b6 The minikube clusters could be created by running: make create-kube-clusters Deploy Gitea \u00b6 The Gitea git repository manager could by deployed by running: make deploy-gitea Note It will take few minutes for Gitea to be installed The Gitea details are stored in the file $DEMO_WORK_DIR/gitea-details.txt . Deploy Argocd \u00b6 The Argocd could by deployed by: make deploy-argocd The Argocd details are stored in the file $DEMO_WORK_DIR/argocd-details.txt . Deploy Gloo Edge and Portal \u00b6 The Gloo Edge enterprise and portal could be deployed via, make deploy-gloo Deploy Tektoncd \u00b6 The Tektoncd could by deployed by: make deploy-pipelines Extras \u00b6 To make the demo builds faster we will use sonatype nexus repository manager, it be deployed by, make deploy-nexus The installation apart from installing the components, it will also download the companion tools such as tkn , gitea , glooctl , kubectl etc., on to $DEMO_HOME/bin . Lets add the tools to the path doing, source $DEMO_WORK_DIR /.envrc","title":"Environment Setup"},{"location":"env-setup/#ensure-poetry","text":"The project uses poetry to setup Python3 virtual environment to run ansible scripts. You can use pipx to install poetry by, pipx install poetry && pipx ensurepath Lets ensure poetry is setup correctly, running the following command should return the version of the installed poetry utility. poetry --version Let us configure poetry to create the python3 virutalenv in the project directory $DEMO_HOME , poetry config virtualenvs.in-project true To create the virutal environment run the following command, poetry install The command will instal all the required python modules in the $DEMO_HOME/.venv . Install the ansible roles and collections that will be used by the playbooks, make install-roles-and-collections Tip","title":"Ensure Poetry"},{"location":"env-setup/#setup-kubernetes-environment","text":"With Ansible environmnt ready we are all good to setup demo environment with required components installed,","title":"Setup Kubernetes Environment"},{"location":"env-setup/#minikube-clusters","text":"The minikube clusters could be created by running: make create-kube-clusters","title":"Minikube clusters"},{"location":"env-setup/#deploy-gitea","text":"The Gitea git repository manager could by deployed by running: make deploy-gitea Note It will take few minutes for Gitea to be installed The Gitea details are stored in the file $DEMO_WORK_DIR/gitea-details.txt .","title":"Deploy Gitea"},{"location":"env-setup/#deploy-argocd","text":"The Argocd could by deployed by: make deploy-argocd The Argocd details are stored in the file $DEMO_WORK_DIR/argocd-details.txt .","title":"Deploy Argocd"},{"location":"env-setup/#deploy-gloo-edge-and-portal","text":"The Gloo Edge enterprise and portal could be deployed via, make deploy-gloo","title":"Deploy Gloo Edge and Portal"},{"location":"env-setup/#deploy-tektoncd","text":"The Tektoncd could by deployed by: make deploy-pipelines","title":"Deploy Tektoncd"},{"location":"env-setup/#extras","text":"To make the demo builds faster we will use sonatype nexus repository manager, it be deployed by, make deploy-nexus The installation apart from installing the components, it will also download the companion tools such as tkn , gitea , glooctl , kubectl etc., on to $DEMO_HOME/bin . Lets add the tools to the path doing, source $DEMO_WORK_DIR /.envrc","title":"Extras"},{"location":"tools-and-sources/","text":"At the end of this chapter you will have the required tools and enviroment ready for running the demo. Download Tools \u00b6 We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows helm brew install helm Install choco install kubernetes-helm yq v4 brew install yq Download Download kustomize brew install kustomize Download choco install kustomize stern brew install stern Download Download vagrant brew install vagrant Download Download Ansible Install Install N.A Important You will need Gloo Mesh Enterprise License Key to run the demo exercises. If you dont have one, get a trial license from solo.io . Demo Sources \u00b6 Clone the demo sources from the GitHub respository, git clone https://github.com/kameshsampath/fruits-api-gitops cd fruits-api-gitops For convinience, we will refer the clone demo sources folder as $DEMO_HOME , export DEMO_HOME = \" $PWD \" Navigate to the project home, cd $DEMO_HOME","title":"Get Started"},{"location":"tools-and-sources/#download-tools","text":"We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows helm brew install helm Install choco install kubernetes-helm yq v4 brew install yq Download Download kustomize brew install kustomize Download choco install kustomize stern brew install stern Download Download vagrant brew install vagrant Download Download Ansible Install Install N.A Important You will need Gloo Mesh Enterprise License Key to run the demo exercises. If you dont have one, get a trial license from solo.io .","title":"Download Tools"},{"location":"tools-and-sources/#demo-sources","text":"Clone the demo sources from the GitHub respository, git clone https://github.com/kameshsampath/fruits-api-gitops cd fruits-api-gitops For convinience, we will refer the clone demo sources folder as $DEMO_HOME , export DEMO_HOME = \" $PWD \" Navigate to the project home, cd $DEMO_HOME","title":"Demo Sources"}]}